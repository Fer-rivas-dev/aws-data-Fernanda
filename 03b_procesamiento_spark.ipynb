{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "007966df-1de7-4e3e-b8ce-bf83374791d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Imports completados\n",
      "📅 2025-10-17 19:41:37\n"
     ]
    }
   ],
   "source": [
    "# ══════════════════════════════════════════════════════════════════\n",
    "# NOTEBOOK 03b: PROCESAMIENTO CON SPARK\n",
    "# Disney Data Pipeline - Fase 3\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError, NoCredentialsError\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✅ Imports completados\")\n",
    "print(f\"📅 {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18136635-fda7-4205-91f3-5855327e5ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Verificando configuración:\n",
      "\n",
      "✅ AWS Access Key       : AKIA...63PU\n",
      "✅ AWS Secret Key       : mNaj...3rkr\n",
      "✅ AWS Region           : us-west-1\n",
      "✅ S3 Bucket            : xideralaws-curso-fernanda\n",
      "\n",
      "✅ Variables configuradas correctamente\n"
     ]
    }
   ],
   "source": [
    "# ══════════════════════════════════════════════════════════════════\n",
    "# CELDA 2: CARGAR VARIABLES DE AMBIENTE\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "required = {\n",
    "    'AWS_ACCESS_KEY_ID': 'AWS Access Key',\n",
    "    'AWS_SECRET_ACCESS_KEY': 'AWS Secret Key',\n",
    "    'AWS_DEFAULT_REGION': 'AWS Region',\n",
    "    'S3_BUCKET_NAME': 'S3 Bucket'\n",
    "}\n",
    "\n",
    "print(\"🔍 Verificando configuración:\\n\")\n",
    "missing = []\n",
    "\n",
    "for var, desc in required.items():\n",
    "    value = os.getenv(var)\n",
    "    if value:\n",
    "        masked = f\"{value[:4]}...{value[-4:]}\" if 'KEY' in var else value\n",
    "        print(f\"✅ {desc:20} : {masked}\")\n",
    "    else:\n",
    "        print(f\"❌ {desc:20} : NO CONFIGURADA\")\n",
    "        missing.append(var)\n",
    "\n",
    "if missing:\n",
    "    raise EnvironmentError(f\"❌ Faltan variables: {missing}\")\n",
    "\n",
    "print(\"\\n✅ Variables configuradas correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5046d0d-b3c2-41d5-8918-1171e535e605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sesión AWS creada\n",
      "   Account ID: 020635523025\n",
      "   Region: us-west-1\n",
      "\n",
      "✅ S3 configurado: xideralaws-curso-fernanda\n"
     ]
    }
   ],
   "source": [
    "# ══════════════════════════════════════════════════════════════════\n",
    "# CELDA 3: CREAR SESIÓN AWS SEGURA\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "\n",
    "def get_aws_session():\n",
    "    \"\"\"Crea sesión AWS segura con validación\"\"\"\n",
    "    try:\n",
    "        session = boto3.Session(\n",
    "            aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),\n",
    "            aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),\n",
    "            region_name=os.getenv('AWS_DEFAULT_REGION')\n",
    "        )\n",
    "        \n",
    "        # Verificar credenciales con STS\n",
    "        sts = session.client('sts')\n",
    "        identity = sts.get_caller_identity()\n",
    "        \n",
    "        print(\"✅ Sesión AWS creada\")\n",
    "        print(f\"   Account ID: {identity['Account']}\")\n",
    "        print(f\"   Region: {session.region_name}\")\n",
    "        \n",
    "        return session\n",
    "    \n",
    "    except NoCredentialsError:\n",
    "        raise Exception(\"❌ Credenciales AWS no encontradas\")\n",
    "    except ClientError as e:\n",
    "        raise Exception(f\"❌ Error de autenticación: {e}\")\n",
    "\n",
    "# Crear sesión y clientes\n",
    "aws_session = get_aws_session()\n",
    "s3_client = aws_session.client('s3')\n",
    "\n",
    "# Configuración S3\n",
    "S3_BUCKET = os.getenv('S3_BUCKET_NAME')\n",
    "S3_RAW_PREFIX = 'disney-project/raw'\n",
    "S3_CLEANED_PREFIX = 'disney-project/cleaned'\n",
    "S3_FINAL_PREFIX = 'disney-project/final'\n",
    "\n",
    "print(f\"\\n✅ S3 configurado: {S3_BUCKET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c7410b7-e460-4d1d-8006-2229de80f14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Función upload_to_s3 definida\n"
     ]
    }
   ],
   "source": [
    "# ══════════════════════════════════════════════════════════════════\n",
    "# CELDA 4: FUNCIÓN PARA SUBIR ARCHIVOS A S3\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "\n",
    "def upload_to_s3(local_file, s3_key):\n",
    "    \"\"\"Sube archivo a S3 con encriptación\"\"\"\n",
    "    try:\n",
    "        s3_client.upload_file(\n",
    "            Filename=local_file,\n",
    "            Bucket=S3_BUCKET,\n",
    "            Key=s3_key,\n",
    "            ExtraArgs={'ServerSideEncryption': 'AES256'}\n",
    "        )\n",
    "        return f\"✅ Subido: s3://{S3_BUCKET}/{s3_key}\"\n",
    "    except Exception as e:\n",
    "        return f\"❌ Error: {e}\"\n",
    "\n",
    "print(\"✅ Función upload_to_s3 definida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed0c5a2d-114b-417d-9a88-9bc415c9ab2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 Iniciando Spark Session...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/17 19:44:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Spark Session creada\n",
      "   Versión: 4.0.1\n",
      "   App Name: Disney Data Pipeline - Fase 3\n",
      "   Master: local[*]\n"
     ]
    }
   ],
   "source": [
    "# ══════════════════════════════════════════════════════════════════\n",
    "# CELDA 5: CREAR SPARK SESSION\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"🔥 Iniciando Spark Session...\\n\")\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Disney Data Pipeline - Fase 3\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"✅ Spark Session creada\")\n",
    "print(f\"   Versión: {spark.version}\")\n",
    "print(f\"   App Name: {spark.sparkContext.appName}\")\n",
    "print(f\"   Master: {spark.sparkContext.master}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55bd70c1-c83d-472b-a3f9-b1f56c82981c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Cargando datos de Fase 2...\n",
      "\n",
      "================================================================================\n",
      "✅ Pickle cargado\n",
      "\n",
      "📋 Contenido:\n",
      "   - df_movies_clean\n",
      "   - df_characters_clean\n",
      "   - df_relations\n",
      "   - metadata\n",
      "\n",
      "✅ DataFrames extraídos:\n",
      "   🎬 Películas: 119 registros, 19 columnas\n",
      "   👥 Personajes: 1,419 registros, 16 columnas\n",
      "   🔗 Relaciones: 1,068 registros, 3 columnas\n"
     ]
    }
   ],
   "source": [
    "# ══════════════════════════════════════════════════════════════════\n",
    "# CELDA 6: CARGAR DATOS DE FASE 2\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"📦 Cargando datos de Fase 2...\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Verificar que existe\n",
    "if not Path('datos_fase2.pkl').exists():\n",
    "    raise FileNotFoundError(\n",
    "        \"❌ No se encontró 'datos_fase2.pkl'\\n\"\n",
    "        \"   Ejecuta primero: 02_limpieza_transformacion.ipynb\"\n",
    "    )\n",
    "\n",
    "# Cargar pickle\n",
    "with open('datos_fase2.pkl', 'rb') as f:\n",
    "    datos_fase2 = pickle.load(f)\n",
    "\n",
    "print(\"✅ Pickle cargado\")\n",
    "print(f\"\\n📋 Contenido:\")\n",
    "for key in datos_fase2.keys():\n",
    "    print(f\"   - {key}\")\n",
    "\n",
    "# Extraer DataFrames\n",
    "df_movies_pandas = datos_fase2['df_movies_clean']\n",
    "df_characters_pandas = datos_fase2['df_characters_clean']\n",
    "df_relations_pandas = datos_fase2['df_relations']\n",
    "\n",
    "print(f\"\\n✅ DataFrames extraídos:\")\n",
    "print(f\"   🎬 Películas: {len(df_movies_pandas):,} registros, {len(df_movies_pandas.columns)} columnas\")\n",
    "print(f\"   👥 Personajes: {len(df_characters_pandas):,} registros, {len(df_characters_pandas.columns)} columnas\")\n",
    "print(f\"   🔗 Relaciones: {len(df_relations_pandas):,} registros, {len(df_relations_pandas.columns)} columnas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f2454aa-340c-4d0c-902a-05c8c6ca2c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚡ Convirtiendo a Spark DataFrames...\n",
      "\n",
      "1️⃣ Convirtiendo películas...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ 119 registros\n",
      "   Columnas: 19\n",
      "\n",
      "2️⃣ Convirtiendo personajes...\n",
      "   ⚠️  Columnas con tipos complejos: ['films', 'shortFilms', 'tvShows', 'videoGames', 'parkAttractions', 'allies', 'enemies']\n",
      "   📝 Convirtiendo a string...\n",
      "   ✅ 1,419 registros\n",
      "   Columnas: 16\n",
      "\n",
      "3️⃣ Convirtiendo relaciones...\n",
      "   ✅ 1,068 registros\n",
      "\n",
      "✅ Conversión completada exitosamente\n"
     ]
    }
   ],
   "source": [
    "# ══════════════════════════════════════════════════════════════════\n",
    "# CELDA 7: CONVERTIR PANDAS → SPARK DATAFRAMES\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"⚡ Convirtiendo a Spark DataFrames...\\n\")\n",
    "\n",
    "# ============================================\n",
    "# 1. PELÍCULAS\n",
    "# ============================================\n",
    "print(\"1️⃣ Convirtiendo películas...\")\n",
    "spark_movies = spark.createDataFrame(df_movies_pandas)\n",
    "print(f\"   ✅ {spark_movies.count():,} registros\")\n",
    "print(f\"   Columnas: {len(spark_movies.columns)}\")\n",
    "\n",
    "# ============================================\n",
    "# 2. PERSONAJES (con manejo de tipos complejos)\n",
    "# ============================================\n",
    "print(\"\\n2️⃣ Convirtiendo personajes...\")\n",
    "\n",
    "# Identificar columnas problemáticas (listas, objetos complejos)\n",
    "problematic_cols = []\n",
    "for col in df_characters_pandas.columns:\n",
    "    sample = df_characters_pandas[col].iloc[0]\n",
    "    if isinstance(sample, (list, dict)):\n",
    "        problematic_cols.append(col)\n",
    "\n",
    "if problematic_cols:\n",
    "    print(f\"   ⚠️  Columnas con tipos complejos: {problematic_cols}\")\n",
    "    print(f\"   📝 Convirtiendo a string...\")\n",
    "    \n",
    "    # Crear copia para modificar\n",
    "    df_chars_clean = df_characters_pandas.copy()\n",
    "    \n",
    "    # Convertir columnas problemáticas a string\n",
    "    for col in problematic_cols:\n",
    "        df_chars_clean[col] = df_chars_clean[col].astype(str)\n",
    "    \n",
    "    # Crear DataFrame Spark con datos limpios\n",
    "    spark_characters = spark.createDataFrame(df_chars_clean)\n",
    "else:\n",
    "    # No hay problemas, conversión directa\n",
    "    spark_characters = spark.createDataFrame(df_characters_pandas)\n",
    "\n",
    "print(f\"   ✅ {spark_characters.count():,} registros\")\n",
    "print(f\"   Columnas: {len(spark_characters.columns)}\")\n",
    "\n",
    "# ============================================\n",
    "# 3. RELACIONES\n",
    "# ============================================\n",
    "if not df_relations_pandas.empty:\n",
    "    print(\"\\n3️⃣ Convirtiendo relaciones...\")\n",
    "    spark_relations = spark.createDataFrame(df_relations_pandas)\n",
    "    print(f\"   ✅ {spark_relations.count():,} registros\")\n",
    "else:\n",
    "    print(\"\\n3️⃣ Sin relaciones para convertir\")\n",
    "    spark_relations = None\n",
    "\n",
    "print(\"\\n✅ Conversión completada exitosamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bbb491e-f26b-41a1-a04d-02959778d434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 CREANDO VISTAS SQL TEMPORALES\n",
      "\n",
      "================================================================================\n",
      "✅ Vistas SQL creadas:\n",
      "   - movies\n",
      "   - characters\n",
      "   - relations\n",
      "\n",
      "📋 Esquema de películas:\n",
      "root\n",
      " |-- film_title: string (nullable = true)\n",
      " |-- brand: string (nullable = true)\n",
      " |-- box_office_revenue: string (nullable = true)\n",
      " |-- opening_revenue: string (nullable = true)\n",
      " |-- release_date: timestamp (nullable = true)\n",
      " |-- opening_revenue_over_total_revenue: long (nullable = true)\n",
      " |-- imdb_score: double (nullable = true)\n",
      " |-- rt_critics_score: long (nullable = true)\n",
      " |-- rt_audience_score: long (nullable = true)\n",
      " |-- release_year: long (nullable = true)\n",
      " |-- release_month: long (nullable = true)\n",
      " |-- release_quarter: long (nullable = true)\n",
      " |-- release_day_of_week: string (nullable = true)\n",
      " |-- box_office_revenue_clean: double (nullable = true)\n",
      " |-- decade: long (nullable = true)\n",
      " |-- decade_label: string (nullable = true)\n",
      " |-- rating_category: string (nullable = true)\n",
      " |-- segment: string (nullable = true)\n",
      " |-- film_title_clean: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ══════════════════════════════════════════════════════════════════\n",
    "# CELDA 8: CREAR VISTAS SQL TEMPORALES\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"📋 CREANDO VISTAS SQL TEMPORALES\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Registrar DataFrames como tablas SQL\n",
    "spark_movies.createOrReplaceTempView(\"movies\")\n",
    "spark_characters.createOrReplaceTempView(\"characters\")\n",
    "if spark_relations is not None:\n",
    "    spark_relations.createOrReplaceTempView(\"relations\")\n",
    "\n",
    "print(\"✅ Vistas SQL creadas:\")\n",
    "print(\"   - movies\")\n",
    "print(\"   - characters\")\n",
    "if spark_relations is not None:\n",
    "    print(\"   - relations\")\n",
    "\n",
    "# Mostrar esquema de películas\n",
    "print(\"\\n📋 Esquema de películas:\")\n",
    "spark_movies.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "641a3290-d796-4fe8-9587-4c9ff3a3f61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚡ ANÁLISIS DISTRIBUIDO CON SPARK SQL\n",
      "\n",
      "================================================================================\n",
      "\n",
      "💰 TOP 10 PELÍCULAS POR REVENUE:\n",
      "--------------------------------------------------------------------------------\n",
      "+------------------------------------------+------------+----------------+-------------------------+\n",
      "|film_title                                |release_year|revenue_millions|segment                  |\n",
      "+------------------------------------------+------------+----------------+-------------------------+\n",
      "|Star Wars: Episode VII - The Force Awakens|2015        |936.66          |Éxito Crítico y Comercial|\n",
      "|Avengers: Endgame                         |2019        |858.37          |Éxito Crítico y Comercial|\n",
      "|Spider-Man: No Way Home                   |2021        |804.79          |Éxito Crítico y Comercial|\n",
      "|Black Panther                             |2018        |700.06          |Éxito Crítico y Comercial|\n",
      "|Avengers: Infinity War                    |2018        |678.82          |Éxito Crítico y Comercial|\n",
      "|The Avengers                              |2012        |623.36          |Éxito Crítico y Comercial|\n",
      "|Star Wars: Episode VIII - The Last Jedi   |2017        |620.18          |Éxito Comercial          |\n",
      "|Incredibles 2                             |2018        |608.58          |Éxito Crítico y Comercial|\n",
      "|The Lion King                             |2019        |543.64          |Éxito Comercial          |\n",
      "|Rogue One: A Star Wars Story              |2016        |532.18          |Éxito Crítico y Comercial|\n",
      "+------------------------------------------+------------+----------------+-------------------------+\n",
      "\n",
      "\n",
      "📅 ANÁLISIS POR DÉCADA:\n",
      "--------------------------------------------------------------------------------\n",
      "+------------+----------+--------------------+----------------------+\n",
      "|decade_label|num_movies|avg_revenue_millions|total_revenue_millions|\n",
      "+------------+----------+--------------------+----------------------+\n",
      "|1980s       |1         |111.54              |111.54                |\n",
      "|1990s       |14        |159.1               |2227.35               |\n",
      "|2000s       |21        |145.41              |3053.68               |\n",
      "|2010s       |63        |320.14              |20169.01              |\n",
      "|2020s       |20        |219.49              |4389.74               |\n",
      "+------------+----------+--------------------+----------------------+\n",
      "\n",
      "\n",
      "🌟 TOP 10 PERSONAJES MÁS POPULARES:\n",
      "--------------------------------------------------------------------------------\n",
      "+--------------+-----------------+---------+------------+-------------------+\n",
      "|name          |total_appearances|num_films|num_tv_shows|popularity_category|\n",
      "+--------------+-----------------+---------+------------+-------------------+\n",
      "|Chip and Dale |23               |6        |17          |Alta               |\n",
      "|Clarabelle Cow|20               |4        |16          |Alta               |\n",
      "|Big Bad Wolf  |16               |3        |13          |Alta               |\n",
      "|José Carioca  |16               |6        |10          |Alta               |\n",
      "|Ariel         |14               |8        |6           |Media              |\n",
      "|Bigfoot       |14               |4        |10          |Media              |\n",
      "|Captain Hook  |12               |6        |6           |Media              |\n",
      "|Jiminy Cricket|12               |5        |7           |Media              |\n",
      "|Beagle Boys   |11               |2        |9           |Media              |\n",
      "|Br'er Bear    |11               |6        |5           |Media              |\n",
      "+--------------+-----------------+---------+------------+-------------------+\n",
      "\n",
      "\n",
      "✅ Análisis SQL completado\n"
     ]
    }
   ],
   "source": [
    "# ══════════════════════════════════════════════════════════════════\n",
    "# CELDA 9: ANÁLISIS CON SPARK SQL\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"⚡ ANÁLISIS DISTRIBUIDO CON SPARK SQL\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ============================================\n",
    "# QUERY 1: Top 10 películas por revenue\n",
    "# ============================================\n",
    "print(\"\\n💰 TOP 10 PELÍCULAS POR REVENUE:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "top_revenue = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        film_title,\n",
    "        release_year,\n",
    "        ROUND(box_office_revenue_clean/1000000, 2) as revenue_millions,\n",
    "        segment\n",
    "    FROM movies\n",
    "    WHERE box_office_revenue_clean IS NOT NULL\n",
    "    ORDER BY box_office_revenue_clean DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "top_revenue.show(10, truncate=False)\n",
    "\n",
    "# ============================================\n",
    "# QUERY 2: Análisis por década\n",
    "# ============================================\n",
    "print(\"\\n📅 ANÁLISIS POR DÉCADA:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "decade_analysis = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        decade_label,\n",
    "        COUNT(*) as num_movies,\n",
    "        ROUND(AVG(box_office_revenue_clean)/1000000, 2) as avg_revenue_millions,\n",
    "        ROUND(SUM(box_office_revenue_clean)/1000000, 2) as total_revenue_millions\n",
    "    FROM movies\n",
    "    WHERE decade_label IS NOT NULL\n",
    "    GROUP BY decade_label\n",
    "    ORDER BY decade_label\n",
    "\"\"\")\n",
    "\n",
    "decade_analysis.show(truncate=False)\n",
    "\n",
    "# ============================================\n",
    "# QUERY 3: Top personajes\n",
    "# ============================================\n",
    "print(\"\\n🌟 TOP 10 PERSONAJES MÁS POPULARES:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "top_characters = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        name,\n",
    "        total_appearances,\n",
    "        num_films,\n",
    "        num_tv_shows,\n",
    "        popularity_category\n",
    "    FROM characters\n",
    "    WHERE total_appearances > 0\n",
    "    ORDER BY total_appearances DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "top_characters.show(10, truncate=False)\n",
    "\n",
    "print(\"\\n✅ Análisis SQL completado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3513981d-ec5d-417c-b615-8c2b565fddb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔗 REALIZANDO JOIN DISTRIBUIDO\n",
      "\n",
      "================================================================================\n",
      "1️⃣ Contando personajes por película...\n",
      "   ✅ 366 películas con personajes identificados\n",
      "\n",
      "2️⃣ Realizando JOIN...\n",
      "   ✅ JOIN completado: 119 registros\n",
      "\n",
      "3️⃣ Top 15 películas con más personajes:\n",
      "--------------------------------------------------------------------------------\n",
      "+-------------------------------+------------+------------+---------------+\n",
      "|film_title                     |release_year|revenue     |character_count|\n",
      "+-------------------------------+------------+------------+---------------+\n",
      "|Ralph Breaks the Internet      |2018        |2.01091711E8|19             |\n",
      "|Alice Through the Looking Glass|2016        |7.7041381E7 |7              |\n",
      "|The Jungle Book                |2016        |3.64001123E8|7              |\n",
      "|Fantasia 2000                  |2000        |6.065542E7  |7              |\n",
      "|Tangled                        |2010        |2.00821936E8|7              |\n",
      "|Zootopia                       |2016        |3.41268248E8|6              |\n",
      "|Home on the Range              |2004        |5.0030461E7 |6              |\n",
      "|Big Hero 6                     |2014        |2.22527828E8|6              |\n",
      "|Planes: Fire & Rescue          |2014        |5.9165787E7 |5              |\n",
      "|Treasure Planet                |2002        |3.8176783E7 |5              |\n",
      "|Planes                         |2013        |9.0288712E7 |5              |\n",
      "|The Hunchback of Notre Dame    |1996        |1.00138851E8|4              |\n",
      "|Toy Story 4                    |2019        |4.34038008E8|4              |\n",
      "|Frozen II                      |2019        |4.77373578E8|4              |\n",
      "|The Emperor's New Groove       |2000        |8.9302687E7 |3              |\n",
      "+-------------------------------+------------+------------+---------------+\n",
      "only showing top 15 rows\n",
      "\n",
      "✅ Dataset enriquecido creado\n"
     ]
    }
   ],
   "source": [
    "# ══════════════════════════════════════════════════════════════════\n",
    "# CELDA 10: JOIN DISTRIBUIDO - PELÍCULAS CON PERSONAJES\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"🔗 REALIZANDO JOIN DISTRIBUIDO\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if spark_relations is not None:\n",
    "    # Paso 1: Contar personajes por película\n",
    "    print(\"1️⃣ Contando personajes por película...\")\n",
    "    \n",
    "    char_count = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            movie_title_clean,\n",
    "            COUNT(DISTINCT character_name) as character_count\n",
    "        FROM relations\n",
    "        GROUP BY movie_title_clean\n",
    "    \"\"\")\n",
    "    \n",
    "    print(f\"   ✅ {char_count.count()} películas con personajes identificados\\n\")\n",
    "    \n",
    "    # Paso 2: JOIN con dataset de películas\n",
    "    print(\"2️⃣ Realizando JOIN...\")\n",
    "    \n",
    "    movies_enriched = spark_movies.join(\n",
    "        char_count,\n",
    "        spark_movies['film_title_clean'] == char_count['movie_title_clean'],\n",
    "        how='left'\n",
    "    ).select(\n",
    "        spark_movies['*'],\n",
    "        F.coalesce(char_count['character_count'], F.lit(0)).alias('character_count')\n",
    "    )\n",
    "    \n",
    "    print(f\"   ✅ JOIN completado: {movies_enriched.count():,} registros\\n\")\n",
    "    \n",
    "    # Mostrar películas con más personajes\n",
    "    print(\"3️⃣ Top 15 películas con más personajes:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    movies_enriched.select(\n",
    "        'film_title',\n",
    "        'release_year',\n",
    "        F.col('box_office_revenue_clean').alias('revenue'),\n",
    "        'character_count'\n",
    "    ).orderBy(F.col('character_count').desc()).show(15, truncate=False)\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️  No hay relaciones disponibles\")\n",
    "    movies_enriched = spark_movies.withColumn('character_count', F.lit(0))\n",
    "\n",
    "print(\"\\n✅ Dataset enriquecido creado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f097424e-2b78-4f66-ba22-fcab581ffd29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 AGREGACIONES POR SEGMENTO\n",
      "\n",
      "================================================================================\n",
      "🎯 MÉTRICAS POR SEGMENTO:\n",
      "+-------------------------+----------+---------------+--------------+--------------+\n",
      "|segment                  |num_movies|total_revenue  |avg_revenue   |avg_characters|\n",
      "+-------------------------+----------+---------------+--------------+--------------+\n",
      "|Éxito Crítico y Comercial|30        |1.4382616427E10|4.7942054757E8|1.0           |\n",
      "|Éxito Crítico            |43        |7.439824875E9  |1.7301918314E8|1.5           |\n",
      "|Bajo Rendimiento         |38        |4.512640573E9  |1.1875369929E8|1.2           |\n",
      "|Éxito Comercial          |8         |3.61623254E9   |4.520290675E8 |0.9           |\n",
      "+-------------------------+----------+---------------+--------------+--------------+\n",
      "\n",
      "\n",
      "✅ Agregación completada: 4 segmentos\n"
     ]
    }
   ],
   "source": [
    "# ══════════════════════════════════════════════════════════════════\n",
    "# CELDA 11: AGREGACIONES POR SEGMENTO\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"📊 AGREGACIONES POR SEGMENTO\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Agregación por segmento\n",
    "agg_by_segment = movies_enriched.groupBy('segment').agg(\n",
    "    F.count('*').alias('num_movies'),\n",
    "    F.round(F.sum('box_office_revenue_clean'), 2).alias('total_revenue'),\n",
    "    F.round(F.avg('box_office_revenue_clean'), 2).alias('avg_revenue'),\n",
    "    F.round(F.avg('character_count'), 1).alias('avg_characters')\n",
    ").orderBy(F.col('total_revenue').desc())\n",
    "\n",
    "print(\"🎯 MÉTRICAS POR SEGMENTO:\")\n",
    "agg_by_segment.show(truncate=False)\n",
    "\n",
    "# Convertir a Pandas\n",
    "df_segment_agg = agg_by_segment.toPandas()\n",
    "\n",
    "print(f\"\\n✅ Agregación completada: {len(df_segment_agg)} segmentos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "267e7237-800e-4741-bbea-fa8776a16871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📅 AGREGACIONES TEMPORALES\n",
      "\n",
      "================================================================================\n",
      "1️⃣ Agregando por año...\n",
      "✅ 33 años procesados\n",
      "\n",
      "📊 Datos por año (2000+):\n",
      "+------------+----------+--------------+-------------+--------------+\n",
      "|release_year|num_movies|avg_revenue   |total_revenue|avg_characters|\n",
      "+------------+----------+--------------+-------------+--------------+\n",
      "|2000        |3         |7.230504433E7 |2.16915133E8 |4.0           |\n",
      "|2001        |2         |1.69964861E8  |3.39929722E8 |0.5           |\n",
      "|2002        |2         |9.19855605E7  |1.83971121E8 |3.0           |\n",
      "|2003        |2         |2.125256275E8 |4.25051255E8 |1.0           |\n",
      "|2004        |2         |1.557357765E8 |3.11471553E8 |3.0           |\n",
      "|2005        |2         |7.74323855E7  |1.54864771E8 |0.0           |\n",
      "|2006        |1         |2.44082982E8  |2.44082982E8 |0.0           |\n",
      "|2007        |2         |1.521339125E8 |3.04267825E8 |1.0           |\n",
      "|2008        |2         |1.689308715E8 |3.37861743E8 |0.0           |\n",
      "|2009        |3         |1.7842030867E8|5.35260926E8 |1.3           |\n",
      "|2010        |4         |3.1561281425E8|1.262451257E9|2.8           |\n",
      "|2011        |5         |1.194446258E8 |5.97223129E8 |0.2           |\n",
      "|2012        |4         |2.713387685E8 |1.085355074E9|0.5           |\n",
      "|2013        |6         |2.6830124067E8|1.609807444E9|1.8           |\n",
      "|2014        |5         |2.23209433E8  |1.116047165E9|2.2           |\n",
      "|2015        |6         |3.7609507333E8|2.25657044E9 |0.7           |\n",
      "|2016        |9         |3.0738890011E8|2.766500101E9|2.3           |\n",
      "|2017        |7         |3.6094702957E8|2.526629207E9|0.1           |\n",
      "|2018        |7         |3.8831139957E8|2.718179797E9|2.9           |\n",
      "|2019        |10        |4.230242223E8 |4.230242223E9|1.2           |\n",
      "|2020        |1         |6.1555145E7   |6.1555145E7  |0.0           |\n",
      "|2021        |7         |2.3068264943E8|1.614778546E9|0.4           |\n",
      "|2022        |5         |2.729387296E8 |1.364693648E9|0.2           |\n",
      "|2023        |7         |1.9267261E8   |1.34870827E9 |0.3           |\n",
      "+------------+----------+--------------+-------------+--------------+\n",
      "\n",
      "✅ Pandas: 33 registros\n",
      "\n",
      "2️⃣ Agregando por década...\n",
      "✅ 5 décadas procesadas\n",
      "\n",
      "📊 Datos por década:\n",
      "+------------+----------+--------------+---------------+\n",
      "|decade_label|num_movies|avg_revenue   |total_revenue  |\n",
      "+------------+----------+--------------+---------------+\n",
      "|1980s       |1         |1.11543479E8  |1.11543479E8   |\n",
      "|1990s       |14        |1.5909660421E8|2.227352459E9  |\n",
      "|2000s       |21        |1.4541319195E8|3.053677031E9  |\n",
      "|2010s       |63        |3.2014294979E8|2.0169005837E10|\n",
      "|2020s       |20        |2.1948678045E8|4.389735609E9  |\n",
      "+------------+----------+--------------+---------------+\n",
      "\n",
      "✅ Pandas: 5 registros\n",
      "\n",
      "✅ CELDA 12 COMPLETADA\n"
     ]
    }
   ],
   "source": [
    "# ══════════════════════════════════════════════════════════════════\n",
    "# CELDA 12: AGREGACIONES TEMPORALES\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"📅 AGREGACIONES TEMPORALES\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ============================================\n",
    "# Por AÑO\n",
    "# ============================================\n",
    "print(\"1️⃣ Agregando por año...\")\n",
    "\n",
    "agg_by_year = movies_enriched.groupBy('release_year').agg(\n",
    "    F.count('*').alias('num_movies'),\n",
    "    F.round(F.avg('box_office_revenue_clean'), 2).alias('avg_revenue'),\n",
    "    F.round(F.sum('box_office_revenue_clean'), 2).alias('total_revenue'),\n",
    "    F.round(F.avg('character_count'), 1).alias('avg_characters')\n",
    ").orderBy('release_year')\n",
    "\n",
    "print(f\"✅ {agg_by_year.count()} años procesados\")\n",
    "\n",
    "# Mostrar datos\n",
    "print(\"\\n📊 Datos por año (2000+):\")\n",
    "agg_by_year.filter(F.col('release_year') >= 2000).show(25, truncate=False)\n",
    "\n",
    "# Convertir a Pandas\n",
    "df_year_agg = agg_by_year.toPandas()\n",
    "print(f\"✅ Pandas: {len(df_year_agg)} registros\\n\")\n",
    "\n",
    "# ============================================\n",
    "# Por DÉCADA\n",
    "# ============================================\n",
    "print(\"2️⃣ Agregando por década...\")\n",
    "\n",
    "# Verificar si existe decade_label\n",
    "if 'decade_label' in movies_enriched.columns:\n",
    "    decade_col = 'decade_label'\n",
    "else:\n",
    "    print(\"   ⚠️  Creando columna decade...\")\n",
    "    movies_enriched = movies_enriched.withColumn(\n",
    "        'decade',\n",
    "        F.floor(F.col('release_year') / 10) * 10\n",
    "    )\n",
    "    decade_col = 'decade'\n",
    "\n",
    "agg_by_decade = movies_enriched.groupBy(decade_col).agg(\n",
    "    F.count('*').alias('num_movies'),\n",
    "    F.round(F.avg('box_office_revenue_clean'), 2).alias('avg_revenue'),\n",
    "    F.round(F.sum('box_office_revenue_clean'), 2).alias('total_revenue')\n",
    ").orderBy(decade_col)\n",
    "\n",
    "print(f\"✅ {agg_by_decade.count()} décadas procesadas\")\n",
    "\n",
    "# Mostrar datos\n",
    "print(\"\\n📊 Datos por década:\")\n",
    "agg_by_decade.show(truncate=False)\n",
    "\n",
    "# Convertir a Pandas\n",
    "df_decade_agg = agg_by_decade.toPandas()\n",
    "print(f\"✅ Pandas: {len(df_decade_agg)} registros\")\n",
    "\n",
    "print(\"\\n✅ CELDA 12 COMPLETADA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "863d6269-83c1-43f4-80dd-07d9542ab025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 GUARDANDO ARCHIVOS PARQUET\n",
      "\n",
      "================================================================================\n",
      "1️⃣ Guardando movies_enriched...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Guardado\n",
      "\n",
      "2️⃣ Guardando agg_by_segment...\n",
      "   ✅ Guardado\n",
      "\n",
      "3️⃣ Guardando agg_temporal...\n",
      "   ✅ Guardado\n",
      "\n",
      "4️⃣ Guardando agg_decade...\n",
      "   ✅ Guardado\n",
      "\n",
      "✅ Todos los archivos Parquet guardados en: ./spark_output/\n"
     ]
    }
   ],
   "source": [
    "# ══════════════════════════════════════════════════════════════════\n",
    "# CELDA 13: GUARDAR EN FORMATO PARQUET\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"💾 GUARDANDO ARCHIVOS PARQUET\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "Path('./spark_output').mkdir(exist_ok=True)\n",
    "\n",
    "print(\"1️⃣ Guardando movies_enriched...\")\n",
    "movies_enriched.write.mode('overwrite').parquet('./spark_output/movies_enriched.parquet')\n",
    "print(\"   ✅ Guardado\")\n",
    "\n",
    "print(\"\\n2️⃣ Guardando agg_by_segment...\")\n",
    "agg_by_segment.write.mode('overwrite').parquet('./spark_output/agg_segment.parquet')\n",
    "print(\"   ✅ Guardado\")\n",
    "\n",
    "print(\"\\n3️⃣ Guardando agg_temporal...\")\n",
    "agg_by_year.write.mode('overwrite').parquet('./spark_output/agg_temporal.parquet')\n",
    "print(\"   ✅ Guardado\")\n",
    "\n",
    "print(\"\\n4️⃣ Guardando agg_decade...\")\n",
    "agg_by_decade.write.mode('overwrite').parquet('./spark_output/agg_decade.parquet')\n",
    "print(\"   ✅ Guardado\")\n",
    "\n",
    "print(\"\\n✅ Todos los archivos Parquet guardados en: ./spark_output/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "897e51c1-ea0e-49c1-b028-e1278ce1ae81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "☁️  EXPORTANDO A CSV Y SUBIENDO A S3\n",
      "\n",
      "================================================================================\n",
      "1️⃣ Exportando movies_enriched...\n",
      "   ✅ Subido: s3://xideralaws-curso-fernanda/disney-project/final/movies_spark.csv\n",
      "\n",
      "2️⃣ Exportando agg_segment...\n",
      "   ✅ Subido: s3://xideralaws-curso-fernanda/disney-project/final/agg_segment.csv\n",
      "\n",
      "3️⃣ Exportando agg_temporal...\n",
      "   ✅ Subido: s3://xideralaws-curso-fernanda/disney-project/final/agg_temporal.csv\n",
      "\n",
      "4️⃣ Exportando agg_decade...\n",
      "   ✅ Subido: s3://xideralaws-curso-fernanda/disney-project/final/agg_decade.csv\n",
      "\n",
      "🎉 Todos los archivos subidos a S3\n",
      "   Ubicación: s3://xideralaws-curso-fernanda/disney-project/final/\n"
     ]
    }
   ],
   "source": [
    "# ══════════════════════════════════════════════════════════════════\n",
    "# CELDA 14: CONVERTIR A CSV Y SUBIR A S3\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"☁️  EXPORTANDO A CSV Y SUBIENDO A S3\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "Path('./data/final').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 1. Movies enriched\n",
    "print(\"1️⃣ Exportando movies_enriched...\")\n",
    "df_movies_final = movies_enriched.toPandas()\n",
    "movies_csv = './data/final/movies_spark.csv'\n",
    "df_movies_final.to_csv(movies_csv, index=False, encoding='utf-8')\n",
    "result = upload_to_s3(movies_csv, f'{S3_FINAL_PREFIX}/movies_spark.csv')\n",
    "print(f\"   {result}\")\n",
    "\n",
    "# 2. Segment\n",
    "print(\"\\n2️⃣ Exportando agg_segment...\")\n",
    "segment_csv = './data/final/agg_segment.csv'\n",
    "df_segment_agg.to_csv(segment_csv, index=False, encoding='utf-8')\n",
    "result = upload_to_s3(segment_csv, f'{S3_FINAL_PREFIX}/agg_segment.csv')\n",
    "print(f\"   {result}\")\n",
    "\n",
    "# 3. Temporal\n",
    "print(\"\\n3️⃣ Exportando agg_temporal...\")\n",
    "temporal_csv = './data/final/agg_temporal.csv'\n",
    "df_year_agg.to_csv(temporal_csv, index=False, encoding='utf-8')\n",
    "result = upload_to_s3(temporal_csv, f'{S3_FINAL_PREFIX}/agg_temporal.csv')\n",
    "print(f\"   {result}\")\n",
    "\n",
    "# 4. Decade\n",
    "print(\"\\n4️⃣ Exportando agg_decade...\")\n",
    "decade_csv = './data/final/agg_decade.csv'\n",
    "df_decade_agg.to_csv(decade_csv, index=False, encoding='utf-8')\n",
    "result = upload_to_s3(decade_csv, f'{S3_FINAL_PREFIX}/agg_decade.csv')\n",
    "print(f\"   {result}\")\n",
    "\n",
    "print(f\"\\n🎉 Todos los archivos subidos a S3\")\n",
    "print(f\"   Ubicación: s3://{S3_BUCKET}/{S3_FINAL_PREFIX}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd6415a3-cf52-4baf-902c-141453c979a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 PREPARANDO DATOS PARA DASHBOARD\n",
      "\n",
      "================================================================================\n",
      "✅ Datos guardados en: datos_fase3.pkl\n",
      "   Películas: 119\n",
      "   Personajes: 1419\n"
     ]
    }
   ],
   "source": [
    "# ══════════════════════════════════════════════════════════════════\n",
    "# CELDA 15: GUARDAR DATOS PARA DASHBOARD\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"💾 PREPARANDO DATOS PARA DASHBOARD\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "datos_fase3 = {\n",
    "    'df_movies_dashboard': df_movies_final,\n",
    "    'df_characters_final': df_characters_pandas,\n",
    "    'segment_analysis': df_segment_agg,\n",
    "    'temporal_analysis': df_year_agg,\n",
    "    'decade_analysis': df_decade_agg,\n",
    "    'rankings': {\n",
    "        'top_revenue': df_movies_final.nlargest(10, 'box_office_revenue_clean'),\n",
    "    },\n",
    "    'metadata': {\n",
    "        'movies_count': len(df_movies_final),\n",
    "        'characters_count': len(df_characters_pandas),\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'notebook': '03b_procesamiento_spark.ipynb',\n",
    "        'spark_version': spark.version,\n",
    "        'status': 'SUCCESS'\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('datos_fase3.pkl', 'wb') as f:\n",
    "    pickle.dump(datos_fase3, f)\n",
    "\n",
    "print(\"✅ Datos guardados en: datos_fase3.pkl\")\n",
    "print(f\"   Películas: {datos_fase3['metadata']['movies_count']}\")\n",
    "print(f\"   Personajes: {datos_fase3['metadata']['characters_count']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b683edc-c664-4aa6-8124-787483cd3392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "🎉 PROCESAMIENTO SPARK COMPLETADO\n",
      "======================================================================\n",
      "\n",
      "⚡ SPARK PROCESSING:\n",
      "   Versión: 4.0.1\n",
      "   Películas: 119\n",
      "   Personajes: 1,419\n",
      "   Relaciones: 1,068\n",
      "\n",
      "📊 ARCHIVOS GENERADOS:\n",
      "   Parquet: ./spark_output/ (4 archivos)\n",
      "   CSV + S3: s3://xideralaws-curso-fernanda/disney-project/final/ (4 archivos)\n",
      "   Pickle: datos_fase3.pkl\n",
      "\n",
      "🚀 SIGUIENTE PASO:\n",
      "   Ejecutar: streamlit run dashboard_disney.py\n",
      "======================================================================\n",
      "\n",
      "✅ Spark Session cerrada\n",
      "✅ Notebook 03b completado al 100%\n"
     ]
    }
   ],
   "source": [
    "# ══════════════════════════════════════════════════════════════════\n",
    "# CELDA 16: RESUMEN FINAL Y CERRAR SPARK\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"🎉 PROCESAMIENTO SPARK COMPLETADO\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n⚡ SPARK PROCESSING:\")\n",
    "print(f\"   Versión: {spark.version}\")\n",
    "print(f\"   Películas: {movies_enriched.count():,}\")\n",
    "print(f\"   Personajes: {spark_characters.count():,}\")\n",
    "if spark_relations:\n",
    "    print(f\"   Relaciones: {spark_relations.count():,}\")\n",
    "\n",
    "print(f\"\\n📊 ARCHIVOS GENERADOS:\")\n",
    "print(f\"   Parquet: ./spark_output/ (4 archivos)\")\n",
    "print(f\"   CSV + S3: s3://{S3_BUCKET}/{S3_FINAL_PREFIX}/ (4 archivos)\")\n",
    "print(f\"   Pickle: datos_fase3.pkl\")\n",
    "\n",
    "print(f\"\\n🚀 SIGUIENTE PASO:\")\n",
    "print(f\"   Ejecutar: streamlit run dashboard_disney.py\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "spark.stop()\n",
    "print(\"\\n✅ Spark Session cerrada\")\n",
    "print(\"✅ Notebook 03b completado al 100%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ec06a5-e3e6-4c03-984f-49c001cd3d2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
