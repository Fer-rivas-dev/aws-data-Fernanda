{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85ff13ab",
   "metadata": {},
   "source": [
    "# NYC Yellow Cab – PySpark Analysis Notebook\n",
    "\n",
    "Este cuaderno descarga datos públicos de Yellow Taxi del TLC de NYC (formato Parquet), los carga con **PySpark** y realiza un análisis exploratorio básico.\n",
    "\n",
    "**Qué incluye:**\n",
    "- Instalación (opcional) de PySpark en caso de que el entorno no lo tenga.\n",
    "- Descarga parametrizable de archivos mensuales (`yellow_tripdata_YYYY-MM.parquet`) desde el CDN oficial del TLC.\n",
    "- Carga a `Spark DataFrame`, revisión de esquema y limpieza básica.\n",
    "- Métricas rápidas (viajes, distancia, importe), histogramas y patrones por hora/día.\n",
    "- *Join* con el catálogo de zonas de taxi para nombres de pickups/dropoffs.\n",
    "- Ejemplos de consultas prácticas.\n",
    "  \n",
    "> **Nota:** Las celdas que descargan datos usan `requests`. Si tu entorno no tiene acceso a internet, primero descarga los archivos y ajústales la ruta local.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5cb8fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /simple/findspark/\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "PySpark instalado.\n"
     ]
    }
   ],
   "source": [
    "# (Opcional) Instala PySpark si no está disponible.\n",
    "# Si ya tienes PySpark, puedes saltarte esta celda.\n",
    "try:\n",
    "    import pyspark  # noqa: F401\n",
    "    print(\"PySpark ya está disponible.\")\n",
    "except ModuleNotFoundError:\n",
    "    %pip install -q pyspark==3.5.1 findspark==2.0.1\n",
    "    import importlib, sys\n",
    "    importlib.invalidate_caches()\n",
    "    print(\"PySpark instalado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8e921da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/14 17:45:15 WARN Utils: Your hostname, dalia resolves to a loopback address: 127.0.1.1; using 192.168.1.83 instead (on interface wlp2s0)\n",
      "25/10/14 17:45:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/14 17:45:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.83:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>NYC Yellow Cab EDA</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f914ce6a9d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importaciones básicas y sesión de Spark\n",
    "import os, sys, math, textwrap, json, gzip, io, time, pathlib\n",
    "from datetime import datetime\n",
    "from typing import List\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession, functions as F, types as T, Window\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"NYC Yellow Cab EDA\")\n",
    "         .config(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "         .getOrCreate())\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2972cf6b",
   "metadata": {},
   "source": [
    "## Parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d70daa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_DIR: /home/dalia/Downloads/data/nyc_taxi\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['2025-06', '2025-05', '2025-04']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Puedes ajustar el año/mes aquí. Por defecto tomamos varios meses recientes.\n",
    "# El TLC publica archivos en: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_YYYY-MM.parquet\n",
    "BASE_URL = \"https://d37ci6vzurychx.cloudfront.net/trip-data\"\n",
    "DATA_DIR = \"./data/nyc_taxi\"  # carpeta local donde guardaremos los archivos\n",
    "ZONE_LOOKUP_URL = \"https://d37ci6vzurychx.cloudfront.net/misc/taxi+_zone_lookup.csv\"\n",
    "\n",
    "# Elige meses a bajar (YYYY-MM). Puedes poner solo uno, p.ej. [\"2025-06\"]\n",
    "MONTHS = [\"2025-06\", \"2025-05\", \"2025-04\"]\n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "print(\"DATA_DIR:\", os.path.abspath(DATA_DIR))\n",
    "MONTHS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a201c1e",
   "metadata": {},
   "source": [
    "## Descarga de archivos (Parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697dfd93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descargando: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2025-06.parquet\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def download_if_not_exists(url: str, dest_path: str, chunk_size: int = 2**20):\n",
    "    p = pathlib.Path(dest_path)\n",
    "    if p.exists() and p.stat().st_size > 0:\n",
    "        print(f\"Ya existe: {p.name} ({p.stat().st_size/1e6:.1f} MB)\")\n",
    "        return dest_path\n",
    "    print(f\"Descargando: {url}\")\n",
    "    with requests.get(url, stream=True, timeout=60) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(dest_path, \"wb\") as f:\n",
    "            for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "    print(f\"Guardado en: {dest_path}\")\n",
    "    return dest_path\n",
    "\n",
    "# Descarga de los meses seleccionados\n",
    "local_parquets = []\n",
    "for ym in MONTHS:\n",
    "    url = f\"{BASE_URL}/yellow_tripdata_{ym}.parquet\"\n",
    "    local_path = os.path.join(DATA_DIR, f\"yellow_tripdata_{ym}.parquet\")\n",
    "    try:\n",
    "        download_if_not_exists(url, local_path)\n",
    "        local_parquets.append(local_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error descargando {ym}: {e}\")\n",
    "\n",
    "print(\"Archivos locales:\", local_parquets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c66e68",
   "metadata": {},
   "source": [
    "## Cargar datos a Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad031f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos todos los Parquet disponibles\n",
    "if not local_parquets:\n",
    "    raise FileNotFoundError(\"No hay archivos Parquet descargados. Ajusta MONTHS o descarga manualmente.\")\n",
    "\n",
    "df = spark.read.parquet(*local_parquets)\n",
    "\n",
    "# Normalizamos nombres de columnas a lower_snake_case por conveniencia\n",
    "for c in df.columns:\n",
    "    df = df.withColumnRenamed(c, c.lower())\n",
    "\n",
    "print(\"Número de filas:\", df.count())\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715f753d",
   "metadata": {},
   "source": [
    "## Limpieza y filtros básicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ad9223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtramos registros con valores imposibles o fuera de rango.\n",
    "# Puedes ajustar umbrales según tus necesidades.\n",
    "df_clean = (\n",
    "    df\n",
    "    .filter(F.col(\"passenger_count\").isNotNull())\n",
    "    .filter((F.col(\"passenger_count\") >= 0) & (F.col(\"passenger_count\") <= 6))\n",
    "    .filter(F.col(\"trip_distance\").isNotNull() & (F.col(\"trip_distance\") >= 0) & (F.col(\"trip_distance\") <= 100))\n",
    "    .filter(F.col(\"fare_amount\").isNotNull() & (F.col(\"fare_amount\") >= 0) & (F.col(\"fare_amount\") <= 1000))\n",
    "    .filter(F.col(\"total_amount\").isNotNull() & (F.col(\"total_amount\") >= -50) & (F.col(\"total_amount\") <= 1500))\n",
    "    .filter(F.col(\"tpep_pickup_datetime\").isNotNull() & F.col(\"tpep_dropoff_datetime\").isNotNull())\n",
    "    .withColumn(\"trip_minutes\", (F.col(\"tpep_dropoff_datetime\").cast(\"timestamp\").cast(\"long\") - F.col(\"tpep_pickup_datetime\").cast(\"timestamp\").cast(\"long\"))/60.0)\n",
    "    .filter((F.col(\"trip_minutes\") > 0) & (F.col(\"trip_minutes\") <= 360))  # hasta 6 horas\n",
    "    .withColumn(\"pickup_date\", F.to_date(\"tpep_pickup_datetime\"))\n",
    "    .withColumn(\"pickup_hour\", F.hour(\"tpep_pickup_datetime\"))\n",
    "    .withColumn(\"pickup_dow\", F.date_format(\"pickup_date\", \"E\"))\n",
    ")\n",
    "\n",
    "df_clean.cache()\n",
    "print(\"Filas después de limpieza:\", df_clean.count())\n",
    "df_clean.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3aacf0",
   "metadata": {},
   "source": [
    "## KPIs rápidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6879be65",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = df_clean.agg(\n",
    "    F.count(\"*\").alias(\"trips\"),\n",
    "    F.mean(\"trip_distance\").alias(\"avg_distance_mi\"),\n",
    "    F.expr(\"percentile_approx(trip_distance, 0.5)\").alias(\"median_distance_mi\"),\n",
    "    F.mean(\"total_amount\").alias(\"avg_total_amount\"),\n",
    "    F.expr(\"percentile_approx(total_amount, 0.5)\").alias(\"median_total_amount\"),\n",
    "    F.mean(\"trip_minutes\").alias(\"avg_trip_minutes\")\n",
    ").toPandas()\n",
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f730770",
   "metadata": {},
   "source": [
    "## Histograma de `trip_distance`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f233a154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para graficar, convertimos una muestra a Pandas\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sample = df_clean.select(\"trip_distance\").sample(False, 0.02, seed=42).toPandas()\n",
    "plt.figure()\n",
    "plt.hist(sample[\"trip_distance\"].dropna(), bins=50)\n",
    "plt.title(\"Distribución de distancia del viaje (millas)\")\n",
    "plt.xlabel(\"trip_distance (mi)\")\n",
    "plt.ylabel(\"frecuencia\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9512b2f6",
   "metadata": {},
   "source": [
    "## Viajes por hora del día"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b196475f",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_hour = (df_clean.groupBy(\"pickup_hour\")\n",
    "           .agg(F.count(\"*\").alias(\"trips\"))\n",
    "           .orderBy(\"pickup_hour\"))\n",
    "\n",
    "pdf = by_hour.toPandas()\n",
    "display(pdf)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(pdf[\"pickup_hour\"], pdf[\"trips\"], marker=\"o\")\n",
    "plt.title(\"Viajes por hora de recogida\")\n",
    "plt.xlabel(\"Hora (0-23)\")\n",
    "plt.ylabel(\"Viajes\")\n",
    "plt.xticks(range(0,24,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94515128",
   "metadata": {},
   "source": [
    "## Distribución por `payment_type`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac80920",
   "metadata": {},
   "outputs": [],
   "source": [
    "payment_map = {0: \"Unknown\", 1: \"Credit card\", 2: \"Cash\", 3: \"No charge\", 4: \"Dispute\", 5: \"Unknown\", 6: \"Voided\"}\n",
    "df_pay = (df_clean.groupBy(\"payment_type\").agg(F.count(\"*\").alias(\"trips\"))).withColumn(\n",
    "    \"payment_desc\",\n",
    "    F.when(F.col(\"payment_type\").isNull(), F.lit(\"Null\"))\n",
    "     .otherwise(F.create_map([F.lit(x) for x in sum(payment_map.items(), ())])[F.col(\"payment_type\")])\n",
    ")\n",
    "\n",
    "pdf = df_pay.orderBy(F.desc(\"trips\")).toPandas()\n",
    "display(pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4437bd04",
   "metadata": {},
   "source": [
    "## Join con catálogo de zonas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627cf658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descargamos el lookup de zonas de taxi (CSV) y lo cargamos con Spark\n",
    "zone_csv_local = os.path.join(DATA_DIR, \"taxi_zone_lookup.csv\")\n",
    "try:\n",
    "    download_if_not_exists(ZONE_LOOKUP_URL, zone_csv_local)\n",
    "except Exception as e:\n",
    "    print(\"No se pudo descargar el catálogo de zonas. Puedes bajarlo manualmente:\", e)\n",
    "\n",
    "zones = (spark.read\n",
    "         .option(\"header\", True)\n",
    "         .csv(zone_csv_local))\n",
    "\n",
    "zones = zones.select(\n",
    "    F.col(\"LocationID\").cast(\"int\").alias(\"LocationID\"),\n",
    "    F.col(\"Borough\").alias(\"borough\"),\n",
    "    F.col(\"Zone\").alias(\"zone\"),\n",
    "    F.col(\"service_zone\").alias(\"service_zone\"),\n",
    ")\n",
    "\n",
    "dfz = (df_clean\n",
    "       .join(zones.withColumnRenamed(\"LocationID\", \"PULocationID\"), on=\"PULocationID\", how=\"left\")\n",
    "       .withColumnRenamed(\"borough\", \"pu_borough\")\n",
    "       .withColumnRenamed(\"zone\", \"pu_zone\")\n",
    "       .withColumnRenamed(\"service_zone\", \"pu_service_zone\")\n",
    "       .join(zones.withColumnRenamed(\"LocationID\", \"DOLocationID\"), on=\"DOLocationID\", how=\"left\")\n",
    "       .withColumnRenamed(\"borough\", \"do_borough\")\n",
    "       .withColumnRenamed(\"zone\", \"do_zone\")\n",
    "       .withColumnRenamed(\"service_zone\", \"do_service_zone\")\n",
    ")\n",
    "\n",
    "dfz.select(\"pu_borough\",\"pu_zone\",\"do_borough\",\"do_zone\",\"trip_distance\",\"total_amount\").limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cc7213",
   "metadata": {},
   "source": [
    "## Zonas con más pickups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c890db93",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_pu = (dfz.groupBy(\"pu_borough\",\"pu_zone\")\n",
    "           .agg(F.count(\"*\").alias(\"trips\"))\n",
    "           .orderBy(F.desc(\"trips\"))\n",
    "           .limit(20))\n",
    "\n",
    "top_pu.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c584b6e",
   "metadata": {},
   "source": [
    "## Velocidad promedio por zona (millas/hora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa1a5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimación simple de velocidad = distancia (mi) / tiempo (h), filtrando outliers\n",
    "df_speed = (dfz\n",
    "            .filter((F.col(\"trip_distance\") > 0) & (F.col(\"trip_minutes\") > 0))\n",
    "            .withColumn(\"mph\", F.col(\"trip_distance\") / (F.col(\"trip_minutes\")/60.0))\n",
    "            .filter((F.col(\"mph\") > 1) & (F.col(\"mph\") < 60)))\n",
    "\n",
    "avg_speed = (df_speed.groupBy(\"pu_borough\",\"pu_zone\")\n",
    "             .agg(F.mean(\"mph\").alias(\"avg_mph\"),\n",
    "                  F.count(\"*\").alias(\"trips\"))\n",
    "             .filter(F.col(\"trips\") >= 100)   # mínimo de observaciones\n",
    "             .orderBy(F.desc(\"avg_mph\")))\n",
    "\n",
    "avg_speed.limit(20).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a3a4b8",
   "metadata": {},
   "source": [
    "## Detección simple de anomalías en tarifas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f07f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regla simple: outliers por total_amount usando IQR\n",
    "q = df_clean.approxQuantile(\"total_amount\", [0.25, 0.75], 0.01)\n",
    "q1, q3 = q\n",
    "iqr = q3 - q1\n",
    "low, high = (q1 - 1.5*iqr), (q3 + 1.5*iqr)\n",
    "\n",
    "anoms = (df_clean\n",
    "         .filter((F.col(\"total_amount\") < low) | (F.col(\"total_amount\") > high))\n",
    "         .select(\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\", \"passenger_count\",\n",
    "                 \"trip_distance\", \"fare_amount\", \"total_amount\", \"PULocationID\", \"DOLocationID\")\n",
    "         .orderBy(F.desc(\"total_amount\"))\n",
    "         .limit(50))\n",
    "\n",
    "anoms.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171987c3",
   "metadata": {},
   "source": [
    "## Guardar muestra limpia a Parquet (opcional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1905e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guarda un subconjunto para compartir o pruebas rápidas\n",
    "output_dir = os.path.join(DATA_DIR, \"clean_sample_parquet\")\n",
    "(df_clean\n",
    " .limit(200000)  # ajusta el tamaño según tu equipo\n",
    " .coalesce(1)\n",
    " .write\n",
    " .mode(\"overwrite\")\n",
    " .parquet(output_dir))\n",
    "\n",
    "print(\"Guardado en:\", os.path.abspath(output_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209695e9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Siguientes pasos\n",
    "- Agregar mapas (unir con shapefiles/GeoJSON y usar `geopandas` fuera de Spark).\n",
    "- Construir *dashboards* (p. ej. con **Streamlit**) o pipelines (con **Airflow**).\n",
    "- Entrenar modelos (tiempo estimado, predicción de propina) usando **Spark MLlib**.\n",
    "\n",
    "¿Necesitas una versión con **Delta Lake** o **AWS S3/EMR/Spark on K8s**? Puedo preparar otra variante.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb7f58c-5d1d-4d38-8e77-bcca9f0bd7a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80404eaa-7b97-4db4-aaf7-729391915837",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec341c83-1347-4ad0-8956-6f2e2bc7ec16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348bf222-5e11-404d-9abc-c266eca58e62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fd2671-8ed1-4a90-b6bc-127ec04cd31c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207ea217-3354-456b-92d1-646ad1a5b3f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fe9ad1-d230-494d-ad84-6bd3076d54a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe72d076-ce2b-4669-bae8-048cda25b4e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
